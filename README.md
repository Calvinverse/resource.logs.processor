# resource.logs.processor

This repository contains the source code for the resource.logs.processor image which contains an
instance of the [Logstash](https://www.elastic.co/products/logstash) log processor.

## Image

The image is created by using the [Linux base image](https://github.com/Calvinverse/base.vm.linux)
and ammending it using a [Chef](https://www.chef.io/chef/) cookbook which installs the Java
Development Kit and Logstash.

There are two different images that can be created. One for use on a Hyper-V server and one for use
in Azure. Which image is created depends on the build command line used.

### Contents

* The Java development kit from the OpenJDK. The version of which is determined by the version of the
  `java` cookbook in the `metadata.rb` file.
* The Logstash files. The version of which is determined by the `default['logstash']['version']`
  attribute in the `default.rb` attributes file in the cookbook.

### Configuration

* Logstash is installed in the `/usr/share/logstash` directory.
* The configuration files for Logstash are stored in the `/etc/logstash` directory.
* Logstash filter configuration files are kept in the `/etc/logstash/conf.d` directory.
* The Logstash metrics are available on port `9600`.

* The filter configuration files are generated by Consul-Template using a double step process which
  first creates a Consul-Template configuration file for every file stored in the Consul K-V under
  the `config/services/logs/filters` path. Once the files are created Consul-Template sends itself
  a SIGHUP signal indicating that a configuration reload is required. This will load the new
  configuration files so that the secrets can be resolved.

### Authentication

Logstash needs a number of credentials. These are:

* Credentials to connect to RabbitMQ. These are obtained via Consul-Template from
  [Vault](https://vaultproject.io).
* The credentials which are used to connect to ElasticSearch are obtained from the Consul K-V, for user
  names, and Vault, for the passwords.

### Provisioning

No changes to the provisioning are applied other than the default one for the base image.

### Logs

The logging configuration for Logstash itself is set to write to syslog so that the Logstash logs
can be processed the same way as other logs are (ironically via Logstash).

### Metrics

Metrics are collected from Logstash and the JVM via the Logstash metrics API and
[Telegraf](https://www.influxdata.com/time-series-platform/telegraf/).

## Build, test and deploy

The build process follows the standard procedure for
[building Calvinverse images](https://www.calvinverse.net/documentation/how-to-build).

### Hyper-V

For building Hyper-V images use the following command line

    msbuild entrypoint.msbuild /t:build /P:ShouldCreateHypervImage=true /P:RepositoryArchive=PATH_TO_ARTIFACTLOCATION

where `PATH_TO_ARTIFACTLOCATION` is the full path to the directory where the base image artifact
file is stored.

In order to run the smoke tests on the generated image run the following command line

    msbuild entrypoint.msbuild /t:test /P:ShouldCreateHypervImage=true


### Azure

For building Azure images use the following command line

    msbuild entrypoint.msbuild /t:build
        /P:ShouldCreateAzureImage=true
        /P:AzureLocation=LOCATION
        /P:AzureClientId=CLIENT_ID
        /P:AzureClientCertPath=CLIENT_CERT_PATH
        /P:AzureSubscriptionId=SUBSCRIPTION_ID
        /P:AzureImageResourceGroup=IMAGE_RESOURCE_GROUP

where:

* `LOCATION` - The azure data center in which the image should be created. Note that this needs to be the same
  region as the location of the base image. If you want to create the image in a different location then you need to
  copy the base image to that region first.
* `CLIENT_ID` - The client ID of the user that [Packer](https://packer.io) will use to
  [authenticate with Azure](https://www.packer.io/docs/builders/azure#azure-active-directory-service-principal).
* `CLIENT_CERT_PATH` - The client certificate which Packer will use to authenticate with Azure
* `SUBSCRIPTION_ID` - The subscription ID in which the image should be created.
* `IMAGE_RESOURCE_GROUP` - The resource group from which the base image will be pulled and in which the new image
  will be placed once the build completes.

For running the smoke tests on the Azure image

    msbuild entrypoint.msbuild /t:test
        /P:ShouldCreateAzureImage=true
        /P:AzureLocation=LOCATION
        /P:AzureClientId=CLIENT_ID
        /P:AzureClientCertPath=CLIENT_CERT_PATH
        /P:AzureSubscriptionId=SUBSCRIPTION_ID
        /P:AzureImageResourceGroup=IMAGE_RESOURCE_GROUP
        /P:AzureTestImageResourceGroup=TEST_RESOURCE_GROUP

where all the arguments are similar to the build arguments and `TEST_RESOURCE_GROUP` points to an Azure resource
group in which the test images are placed. Note that this resource group needs to be cleaned out after successful
tests have been run because Packer will in that case create a new image.

## Deploy

### Environment

Prior to the provisioning of a new Logstash host the following information should be available in
the environment in which the Logstash instance will be created.

* An Elasticsearch instance needs to be available. The hostname for the Elasticsearch instances
  should be available in the Consul K-V under the path as specified in the filter files.

Make sure the following keys exist in the Consul key-value store:

* `config/services/logs/filters` - Add an entry for each filter file where the file name is the key
  and the content is the value.

Finally add the secrets to Vault

* Provide access to the correct secrets as defined in the filter files via the Consul-Template
  permissions.

### Image provisioning

#### Hyper-V

* Download the new image to the Hyper-V hosts. Ideally there will be multiple hosts of the resource
  for redundancy purposes.
* Create a directory for the image and copy the image VHDX file there.
* Create a VM that points to the image VHDX file with the following settings
  * Generation: 2
  * RAM: 4096 Mb. Do *not* use dynamic memory
  * Network: VM
  * Hard disk: Use existing. Copy the path to the VHDX file
* Update the VM settings:
  * Enable secure boot. Use the Microsoft UEFI Certificate Authority
  * Set the number of CPUs to 2
  * Attach the additional HDD
  * Attach a DVD image that points to an ISO file containing the settings for the environment. These
    are normally found in the output of the [Calvinverse.Infrastructure](https://github.com/Calvinverse/calvinverse.infrastructure)
    repository. Pick the correct ISO for the task, in this case the `Linux Consul Client` image
  * Disable checkpoints
  * Set the VM to always start
  * Set the VM to shut down on stop
* Start the VM, it should automatically connect to the correct environment once it has provisioned
* Once the machine is connected to the environment wait for about 1 - 2 minutes and then provide the
  machine with credentials for Consul-Template so that it can configure Logstash with the appropriate
  secrets. Once the configuration is done Logstash will connect to RabbitMQ and will start processing
  logs.
* Once Logstash has activated and has started processing logs the old VM can be stopped and removed
  * Shut down the Logstash service
    * SSH into the host
    * Disconnect Logstash by stopping the service: `sudo systemctl stop logstash`
    * Issue the `consul leave` command
    * Shut the machine down with the `sudo shutdown now` command
  * Once the machine has stopped, delete it

#### Azure

The easiest way to deploy the Azure images into a cluster on Azure is to use the terraform scripts
provided by the [Azure logs diagnositcs](https://github.com/Calvinverse/infrastructure.azure.observability.logs)
repository. Those scripts will create an Elasticsearch cluster of the suitable size and add a single instance
of a node with the Kibana enabled. Additionally a cluster of Logstash nodes will be provisioned.

## Usage

Once the resource is started and provided with the correct permissions to retrieve information
from [Vault](https://vaultproject.io) it will automatically start processing logs.
